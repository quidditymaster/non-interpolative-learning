\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{jmlr2e}

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

\usepackage{lastpage}
\jmlrheading{23}{2022}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Author One and Author Two}

% Short headings should be running head and authors last names

\ShortHeadings{Non-Interpolative Learning}{Anderton}
\firstpageno{1}

\begin{document}

\title{Emergent Non-Interpolative Learning in Deep Networks}

\author{\name Timothy Anderton \email timbo@blah.com \\
       \addr \\
       %University of Washington\\
       Salt Lake City, UT 84102, USA
}
      %  \AND
      %  \name Author Two \email two@cs.berkeley.edu \\
      %  \addr Division of Computer Science\\
      %  University of California\\
      %  Berkeley, CA 94720-1776, USA}

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
  Some aspects of the learning behavior of deep neural networks can be explained by viewing the network as an interpolator of the labels in its training data. However the out of distribution generalization properties of deep networks are difficult to explain from the perspective of interpolation. In particular this paper explores the generalization of standard deep networks trained on a data stream with consistent distributional drift such that no data point is associated to the same label twice. The task is equivalent to instance counting posed as a multi-class classification task. Generalization on such a data stream cannot be achieved by interpolative learning since predictions which would perform well on the historical training data will never perform well on future data and vice versa. Because of this structure interpolative learners should perform worse than the global average prediction. However deep networks trained with standard modern optimizers such as Adam can adapt to the label distributional drift and learn to assign higher probabilities to the correct labels than to the labels that have been previously been associated with each input in the training data. 
  Such behavior cannot be explained purely in terms of interpolative data memorization and requires analysis of the associated learning dynamics.  
\end{abstract}

\begin{keywords}
  Generalization, Deep Learning
\end{keywords}

\section{Introduction}

Perhaps the most common mathematical framing of a learning problem is to consider data drawn from some unknown joint probability distribution $p(x, y)$. The $x$ are observed and the task is to somehow estimate either conditional probabilities $p(y | x)$

One of the central surprises of deep learning is that the class of functions which can be represented by any particular 
One partial explanation for the effectiveness of deep neural networks is that because of their huge representation capacity they can effectively memorize their training data. 
Networks acting in the regime of label memorization are sometimes said to be acting as label interpolators. 
Generalization of an interpolating function can be guaranteed if training data sufficiently densely covers the input domain and the relationship between the input domain and the target labels doesn't significantly shift between training time and inference time. 

However it is important to point out that 

In the mathematical sense a function $\hat{y} = f(x)$ is said to interpolate a given set of sample points $(x_i, y_i)$ if $f(x_i) = y_i$ for all available $x_i$ and $y_i$. In general the y values may be a noisy function of x so an exact interpolation in this sense may not always be possible if there are two observations with $x_i=x_j$ but $y_i\neq y_j$ values. 
In a slightly more relaxed sense any function which well approximates the observed data $y_i \approx f(x_i)$ may still be said to interpolate the data. What it means for the data to be well approximated is often codified by picking some sort of loss function $L(y_i, f(x_i)) \in \mathbb{R}$ the mean expectation over a target data distribution is being minimized. 
Most interpolation schemes are designed such that for any set of data there is guaranteed to be only a single function in the interpolating function class which achieves the global optimum 
Most exact interpolation schemes have a direct correspondence to a particular choice of functional family and loss. For example a common class of interpolators is the family of piecewise linear functions. The usual 1D linear interpolating function 

function be made unique there is usuall 
Generally the search is over a family of parameterized functions $f(x) = g(x, \theta)$. 



and new data points are then implicitly mapped onto archetypes from the training data whose associated labels have been memorized. 

Deep learning models have shown a remarkable tendency to improve in terms of generalization power as the network size increases. This tendency is an apparent paradox since the overwhelming majority of functions representable by over-parameterized neural networks must be poorly generalizing. This can be seen to be true since even after learning to memorize the training data 


One common theory of the predictive power of deep neural networks is that they act 
Deep neural networks often show a surprising capability to generalize to data which lies outside that which they have previously been exposed to. 

Learning is often framed in terms of finding 
A common learning problems is to find a function $f$ of some input $x$ such that $f(x)$ is a good approximation of some associated target $y$.     
One common theoretical framing of learning algorithms is to consider them as ways of selecting a function $f$ out of a possible family of functions $F$ such that some appropriate loss $\mathcal{L}(f, d)$ has a low expectation $E_{d \sim D}[\mathcal{L}(f, d)]$ when evaluated over a test dat adistribution $D$. 

encode a task in terms of an objective performance metric
When analyzing deep neural networks from a theoretical perspective it is often useful to consider the network architecture as fixing a family of functions parameterized by the network weights. The 

Here is a citation \cite{chow:68}.

% Acknowledgements and Disclosure of Funding should go at the end, before appendices and references

% \acks{All acknowledgements go at the end of the paper before appendices and references.
% Moreover, you are required to declare funding (financial activities supporting the
% submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found on the JMLR website.}

% % Manual newpage inserted to improve layout of sample file - not
% % needed in general before appendices/bibliography.

% \newpage

% \appendix
% \section*{Appendix A.}
% \label{app:theorem}

% % Note: in this sample, the section number is hard-coded in. Following
% % proper LaTeX conventions, it should properly be coded as a reference:

% %In this appendix we prove the following theorem from
% %Section~\ref{sec:textree-generalization}:

% In this appendix we prove the following theorem from
% Section~6.2:

% \noindent
% {\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
% not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
% dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
% which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
% respective empirical mutual information values based on the sample
% $\dataset$. Then
% \[
% 	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
% \]
% with equality only if $u$ is identically 0.} \hfill\BlackBox

% \noindent
% {\bf Proof}. We use the notation:
% \[
% P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
% P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
% \]
% These values represent the (empirical) probabilities of $v$
% taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
% by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

% {\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{references}

\end{document}
